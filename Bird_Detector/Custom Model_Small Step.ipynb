{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2661430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipynb in c:\\users\\jeromey\\anaconda3\\envs\\iot\\lib\\site-packages (0.5.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "408e8490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from skimage import io\n",
    "from tensorflow.keras import Input, layers\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "from tensorflow.lite.experimental.microfrontend.python.ops import audio_microfrontend_op as frontend_op\n",
    "print(tf.__version__)\n",
    "import numpy as np\n",
    "import shutil, os\n",
    "import matplotlib.pyplot as plt\n",
    "import pathlib\n",
    "import random\n",
    "import sys\n",
    "import io\n",
    "import os\n",
    "import glob\n",
    "from datetime import datetime as dt\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Activation, Flatten, BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2D, DepthwiseConv2D, AveragePooling2D, MaxPooling2D\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import os\n",
    "from absl import app\n",
    "\n",
    "assert tf.__version__.startswith('2')\n",
    "%run functions.ipynb #pulling in functions from other file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad1975a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "91ccfff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_LABELS=2 #Binary classification so 2, [bird:0, unknown:1]\n",
    "IMG_SIZE=IMAGE_SIZE=96 #image resolution is 96x96 dictated by pico\n",
    "EPOCHS=100 #Number of epochs to train for\n",
    "BATCH_SIZE = 64 #Batch for speed and regularization\n",
    "INPUT_SHAPE= (IMG_SIZE,IMG_SIZE,1)\n",
    "norm_layer = tf.keras.layers.Normalization(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfcfbcc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9d7fae6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 458517 images belonging to 2 classes.\n",
      "Found 50945 images belonging to 2 classes.\n",
      "{'notperson': 0, 'person': 1}\n"
     ]
    }
   ],
   "source": [
    "BASE_DIR =\"/Users/Jeromey/IOT/project2/data/augmented\"\n",
    "validation_split = 0.1\n",
    "datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rotation_range=10,\n",
    "    width_shift_range=0.05,\n",
    "    height_shift_range=0.05,\n",
    "    zoom_range=.1,\n",
    "    horizontal_flip=True,\n",
    "    validation_split=validation_split,\n",
    "    #rescale=1. / 255\n",
    ")\n",
    "train_generator = datagen.flow_from_directory(\n",
    "    BASE_DIR,\n",
    "    target_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    subset='training',\n",
    "    color_mode='grayscale') # was 'rgb'\n",
    "val_generator = datagen.flow_from_directory(\n",
    "    BASE_DIR,\n",
    "    target_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    subset='validation',\n",
    "    color_mode='grayscale') # was 'rgb'\n",
    "print(train_generator.class_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adea9882",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38907951",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "62a52914",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8d334bc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"attempt_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_20 (Conv2D)          (None, 47, 47, 8)         80        \n",
      "                                                                 \n",
      " depthwise_conv2d_20 (Depthw  (None, 47, 47, 8)        80        \n",
      " iseConv2D)                                                      \n",
      "                                                                 \n",
      " conv2d_21 (Conv2D)          (None, 45, 45, 16)        1168      \n",
      "                                                                 \n",
      " depthwise_conv2d_21 (Depthw  (None, 45, 45, 16)       160       \n",
      " iseConv2D)                                                      \n",
      "                                                                 \n",
      " max_pooling2d_8 (MaxPooling  (None, 22, 22, 16)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 22, 22, 16)        0         \n",
      "                                                                 \n",
      " conv2d_22 (Conv2D)          (None, 20, 20, 32)        4640      \n",
      "                                                                 \n",
      " depthwise_conv2d_22 (Depthw  (None, 20, 20, 32)       320       \n",
      " iseConv2D)                                                      \n",
      "                                                                 \n",
      " conv2d_23 (Conv2D)          (None, 18, 18, 16)        4624      \n",
      "                                                                 \n",
      " depthwise_conv2d_23 (Depthw  (None, 18, 18, 16)       160       \n",
      " iseConv2D)                                                      \n",
      "                                                                 \n",
      " conv2d_24 (Conv2D)          (None, 16, 16, 8)         1160      \n",
      "                                                                 \n",
      " depthwise_conv2d_24 (Depthw  (None, 16, 16, 8)        80        \n",
      " iseConv2D)                                                      \n",
      "                                                                 \n",
      " max_pooling2d_9 (MaxPooling  (None, 8, 8, 8)          0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten_4 (Flatten)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 2)                 514       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 144,314\n",
      "Trainable params: 144,314\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential([\n",
    "    layers.Input(shape=INPUT_SHAPE),\n",
    " #   norm_layer,\n",
    "    \n",
    "    \n",
    "    \n",
    "    layers.Conv2D(8, 3,2, activation='relu'),\n",
    "    layers.DepthwiseConv2D(kernel_size=(3,3), padding='same'),\n",
    "\n",
    "\n",
    "                        \n",
    "    layers.Conv2D(16, 3, activation='relu'),\n",
    "    layers.DepthwiseConv2D(kernel_size=(3,3), padding='same'),\n",
    "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    layers.Dropout(0.4), \n",
    "    \n",
    "    layers.Conv2D(32, 3, activation='relu'),\n",
    "    layers.DepthwiseConv2D(kernel_size=(3,3), padding='same'),\n",
    "    #layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "                        \n",
    "    layers.Conv2D(16, 3, activation='relu'),\n",
    "    layers.DepthwiseConv2D(kernel_size=(3,3), padding='same'),\n",
    "  #  layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "    layers.Conv2D(8, 3, activation='relu'),\n",
    "    layers.DepthwiseConv2D(kernel_size=(3,3), padding='same'),\n",
    "    layers.MaxPooling2D(pool_size=(2, 2)),                   \n",
    "    \n",
    "    \n",
    "    layers.Flatten(),\n",
    "\n",
    "    layers.Dense(256),\n",
    "    \n",
    "    layers.Dense(2, activation='softmax'),\n",
    "], name=\"attempt_1\")\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ac016098",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "patience=5\n",
    "\n",
    "\n",
    "steps=256\n",
    "model.compile(\n",
    "      optimizer=tf.keras.optimizers.Adam(),\n",
    "      loss='categorical_crossentropy',\n",
    "      metrics=['accuracy'])\n",
    "\n",
    "\n",
    "callback=tf.keras.callbacks.EarlyStopping(\n",
    "    monitor= 'val_accuracy',\n",
    "    verbose=1,\n",
    "    patience=15,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "361a3644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "256/256 [==============================] - 38s 145ms/step - loss: 0.6413 - accuracy: 0.6036 - val_loss: 0.6332 - val_accuracy: 0.6278\n",
      "Epoch 2/50\n",
      "256/256 [==============================] - 37s 144ms/step - loss: 0.5182 - accuracy: 0.7330 - val_loss: 0.5927 - val_accuracy: 0.6757\n",
      "Epoch 3/50\n",
      "256/256 [==============================] - 37s 143ms/step - loss: 0.4766 - accuracy: 0.7601 - val_loss: 0.6245 - val_accuracy: 0.6805\n",
      "Epoch 4/50\n",
      "256/256 [==============================] - 37s 144ms/step - loss: 0.4615 - accuracy: 0.7678 - val_loss: 0.5917 - val_accuracy: 0.7017\n",
      "Epoch 5/50\n",
      "256/256 [==============================] - 37s 144ms/step - loss: 0.4374 - accuracy: 0.7850 - val_loss: 0.5824 - val_accuracy: 0.7234\n",
      "Epoch 6/50\n",
      "256/256 [==============================] - 37s 144ms/step - loss: 0.4255 - accuracy: 0.7941 - val_loss: 0.5808 - val_accuracy: 0.7269\n",
      "Epoch 7/50\n",
      "256/256 [==============================] - 37s 143ms/step - loss: 0.4110 - accuracy: 0.8065 - val_loss: 0.5747 - val_accuracy: 0.7032\n",
      "Epoch 8/50\n",
      "256/256 [==============================] - 37s 143ms/step - loss: 0.4072 - accuracy: 0.8063 - val_loss: 0.5540 - val_accuracy: 0.7258\n",
      "Epoch 9/50\n",
      "256/256 [==============================] - 37s 145ms/step - loss: 0.4000 - accuracy: 0.8127 - val_loss: 0.5615 - val_accuracy: 0.7332\n",
      "Epoch 10/50\n",
      "256/256 [==============================] - 37s 146ms/step - loss: 0.3851 - accuracy: 0.8188 - val_loss: 0.5768 - val_accuracy: 0.7203\n",
      "Epoch 11/50\n",
      "256/256 [==============================] - 37s 146ms/step - loss: 0.3698 - accuracy: 0.8295 - val_loss: 0.5689 - val_accuracy: 0.7388\n",
      "Epoch 12/50\n",
      "256/256 [==============================] - 41s 160ms/step - loss: 0.3713 - accuracy: 0.8250 - val_loss: 0.5529 - val_accuracy: 0.7285\n",
      "Epoch 13/50\n",
      "256/256 [==============================] - 44s 173ms/step - loss: 0.3638 - accuracy: 0.8306 - val_loss: 0.5567 - val_accuracy: 0.7401\n",
      "Epoch 14/50\n",
      "256/256 [==============================] - 44s 170ms/step - loss: 0.3519 - accuracy: 0.8370 - val_loss: 0.5663 - val_accuracy: 0.7296\n",
      "Epoch 15/50\n",
      "256/256 [==============================] - 37s 143ms/step - loss: 0.3606 - accuracy: 0.8387 - val_loss: 0.5563 - val_accuracy: 0.7220\n",
      "Epoch 16/50\n",
      "256/256 [==============================] - 37s 145ms/step - loss: 0.3500 - accuracy: 0.8377 - val_loss: 0.5622 - val_accuracy: 0.7457\n",
      "Epoch 17/50\n",
      "256/256 [==============================] - 52s 204ms/step - loss: 0.3468 - accuracy: 0.8409 - val_loss: 0.6205 - val_accuracy: 0.7396\n",
      "Epoch 18/50\n",
      "256/256 [==============================] - 93s 364ms/step - loss: 0.3475 - accuracy: 0.8394 - val_loss: 0.5312 - val_accuracy: 0.7440\n",
      "Epoch 19/50\n",
      "256/256 [==============================] - 110s 430ms/step - loss: 0.3390 - accuracy: 0.8469 - val_loss: 0.5815 - val_accuracy: 0.7403\n",
      "Epoch 20/50\n",
      "256/256 [==============================] - 98s 382ms/step - loss: 0.3395 - accuracy: 0.8436 - val_loss: 0.5627 - val_accuracy: 0.7256\n",
      "Epoch 21/50\n",
      "256/256 [==============================] - 88s 345ms/step - loss: 0.3355 - accuracy: 0.8455 - val_loss: 0.5771 - val_accuracy: 0.7393\n",
      "Epoch 22/50\n",
      "256/256 [==============================] - 68s 267ms/step - loss: 0.3290 - accuracy: 0.8498 - val_loss: 0.5905 - val_accuracy: 0.7414\n",
      "Epoch 23/50\n",
      "256/256 [==============================] - 75s 295ms/step - loss: 0.3246 - accuracy: 0.8517 - val_loss: 0.5751 - val_accuracy: 0.7386\n",
      "Epoch 24/50\n",
      "256/256 [==============================] - 77s 299ms/step - loss: 0.3306 - accuracy: 0.8529 - val_loss: 0.5135 - val_accuracy: 0.7431\n",
      "Epoch 25/50\n",
      "256/256 [==============================] - 74s 289ms/step - loss: 0.3157 - accuracy: 0.8583 - val_loss: 0.5797 - val_accuracy: 0.7334\n",
      "Epoch 26/50\n",
      "256/256 [==============================] - 70s 272ms/step - loss: 0.3178 - accuracy: 0.8550 - val_loss: 0.5517 - val_accuracy: 0.7418\n",
      "Epoch 27/50\n",
      "256/256 [==============================] - 68s 265ms/step - loss: 0.3173 - accuracy: 0.8583 - val_loss: 0.5622 - val_accuracy: 0.7461\n",
      "Epoch 28/50\n",
      "256/256 [==============================] - 67s 260ms/step - loss: 0.3227 - accuracy: 0.8544 - val_loss: 0.5581 - val_accuracy: 0.7434\n",
      "Epoch 29/50\n",
      "256/256 [==============================] - 67s 261ms/step - loss: 0.3084 - accuracy: 0.8625 - val_loss: 0.5623 - val_accuracy: 0.7416\n",
      "Epoch 30/50\n",
      "256/256 [==============================] - 67s 262ms/step - loss: 0.3160 - accuracy: 0.8602 - val_loss: 0.5509 - val_accuracy: 0.7341\n",
      "Epoch 31/50\n",
      "256/256 [==============================] - 61s 239ms/step - loss: 0.3005 - accuracy: 0.8653 - val_loss: 0.5988 - val_accuracy: 0.7446\n",
      "Epoch 32/50\n",
      "256/256 [==============================] - 51s 201ms/step - loss: 0.3003 - accuracy: 0.8623 - val_loss: 0.6249 - val_accuracy: 0.7307\n",
      "Epoch 33/50\n",
      "256/256 [==============================] - 50s 196ms/step - loss: 0.3026 - accuracy: 0.8671 - val_loss: 0.5477 - val_accuracy: 0.7322\n",
      "Epoch 34/50\n",
      "256/256 [==============================] - 50s 196ms/step - loss: 0.3050 - accuracy: 0.8650 - val_loss: 0.5840 - val_accuracy: 0.7432\n",
      "Epoch 35/50\n",
      "256/256 [==============================] - 50s 194ms/step - loss: 0.3016 - accuracy: 0.8622 - val_loss: 0.5277 - val_accuracy: 0.7392\n",
      "Epoch 36/50\n",
      "256/256 [==============================] - 49s 190ms/step - loss: 0.2966 - accuracy: 0.8692 - val_loss: 0.5407 - val_accuracy: 0.7416\n",
      "Epoch 37/50\n",
      "256/256 [==============================] - 52s 204ms/step - loss: 0.3001 - accuracy: 0.8643 - val_loss: 0.5696 - val_accuracy: 0.7433\n",
      "Epoch 38/50\n",
      "256/256 [==============================] - 60s 233ms/step - loss: 0.2870 - accuracy: 0.8740 - val_loss: 0.6827 - val_accuracy: 0.7471\n",
      "Epoch 39/50\n",
      "256/256 [==============================] - 61s 239ms/step - loss: 0.2906 - accuracy: 0.8751 - val_loss: 0.5427 - val_accuracy: 0.7478\n",
      "Epoch 40/50\n",
      "256/256 [==============================] - 57s 225ms/step - loss: 0.3004 - accuracy: 0.8685 - val_loss: 0.5349 - val_accuracy: 0.7402\n",
      "Epoch 41/50\n",
      "256/256 [==============================] - 47s 184ms/step - loss: 0.2922 - accuracy: 0.8725 - val_loss: 0.5372 - val_accuracy: 0.7431\n",
      "Epoch 42/50\n",
      "256/256 [==============================] - 47s 182ms/step - loss: 0.2946 - accuracy: 0.8712 - val_loss: 0.6375 - val_accuracy: 0.7383\n",
      "Epoch 43/50\n",
      "256/256 [==============================] - 47s 185ms/step - loss: 0.2865 - accuracy: 0.8760 - val_loss: 0.5725 - val_accuracy: 0.7506\n",
      "Epoch 44/50\n",
      "256/256 [==============================] - 49s 191ms/step - loss: 0.2843 - accuracy: 0.8732 - val_loss: 0.6088 - val_accuracy: 0.7452\n",
      "Epoch 45/50\n",
      "256/256 [==============================] - 49s 191ms/step - loss: 0.2821 - accuracy: 0.8742 - val_loss: 0.5678 - val_accuracy: 0.7412\n",
      "Epoch 46/50\n",
      "256/256 [==============================] - 46s 181ms/step - loss: 0.2825 - accuracy: 0.8779 - val_loss: 0.5847 - val_accuracy: 0.7417\n",
      "Epoch 47/50\n",
      "256/256 [==============================] - 52s 205ms/step - loss: 0.2872 - accuracy: 0.8726 - val_loss: 0.5511 - val_accuracy: 0.7526\n",
      "Epoch 48/50\n",
      "256/256 [==============================] - 56s 219ms/step - loss: 0.2845 - accuracy: 0.8742 - val_loss: 0.6242 - val_accuracy: 0.7363\n",
      "Epoch 49/50\n",
      "256/256 [==============================] - 52s 204ms/step - loss: 0.2744 - accuracy: 0.8810 - val_loss: 0.5777 - val_accuracy: 0.7469\n",
      "Epoch 50/50\n",
      "256/256 [==============================] - 50s 196ms/step - loss: 0.2739 - accuracy: 0.8807 - val_loss: 0.5395 - val_accuracy: 0.7445\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_generator, \n",
    "    validation_data=val_generator,  \n",
    "    steps_per_epoch=steps,\n",
    "    validation_steps=steps,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "85948a73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bird accuracy: 0.83\n",
      "Unknown accuracy: 0.76\n"
     ]
    }
   ],
   "source": [
    "test_model(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4382db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\")\n",
    "save_model()\n",
    "print(\"\\n\")\n",
    "plot_accuracy(history)\n",
    "print(\"\\n\")\n",
    "test_model(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1aff7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc656a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss, val_acc = model.evaluate(validation_generator, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c09affb",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR =\"/Users/Jeromey/IOT/project2/data/train/p\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e16d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = tf.keras.models.load_model(argv[1])\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "# with tf.io.gfile.GFile('vww_96_float.tflite', 'wb') as float_file:\n",
    "#   float_file.write(tflite_model)\n",
    "\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "def representative_dataset_gen():\n",
    "  dataset_dir = os.path.join(BASE_DIR, \"person\")\n",
    "  for idx, image_file in enumerate(os.listdir(dataset_dir)):\n",
    "    # 10 representative images should be enough for calibration.\n",
    "    if idx > 10000:\n",
    "        return\n",
    "    full_path = os.path.join(dataset_dir, image_file)\n",
    "    if os.path.isfile(full_path):\n",
    "      img = tf.keras.preprocessing.image.load_img(\n",
    "          full_path, color_mode=\"grayscale\").resize((96, 96))\n",
    "      arr = tf.keras.preprocessing.image.img_to_array(img)\n",
    "      # Scale input to [0, 1.0] like in training.\n",
    "      yield [arr.reshape(1, 96, 96, 1)]\n",
    "      #yield [arr.reshape(1, 96, 96, 1) / 255.] \n",
    "\n",
    "# Convert model to full-int8 and save as quantized tflite flatbuffer.\n",
    "converter.representative_dataset = representative_dataset_gen\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "converter.inference_input_type = tf.int8\n",
    "converter.inference_output_type = tf.int8\n",
    "quantized_tflite_model = converter.convert()\n",
    "with tf.io.gfile.GFile('vww_96_int8_SuperTry.tflite', 'wb') as quantized_file:\n",
    "    quantized_file.write(quantized_tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1d706c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a5d5ae26",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1=tf.keras.utils.load_img(\n",
    "    path='data/train/p/person/30000.jpeg',\n",
    "    color_mode=\"grayscale\",\n",
    "    target_size=(IMG_SIZE,IMG_SIZE),\n",
    "    interpolation='nearest'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d9f249db",
   "metadata": {},
   "outputs": [],
   "source": [
    "x2=tf.keras.utils.load_img(\n",
    "    path='data/train/n/notperson/30000.jpeg',\n",
    "    color_mode=\"grayscale\",\n",
    "    target_size=(IMG_SIZE,IMG_SIZE),\n",
    "    interpolation='nearest'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5fb41bb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.49701282, 0.5029872 ]], dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1_arr=tf.keras.utils.img_to_array(x1)\n",
    "x1_shape=x1_arr.reshape((1,) +x1_arr.shape)\n",
    "x1_predict=model.predict(x1_shape)\n",
    "x1_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8f133561",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.49853185, 0.5014681 ]], dtype=float32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2_arr=tf.keras.utils.img_to_array(x2)\n",
    "x2_shape=x2_arr.reshape((1,) +x2_arr.shape)\n",
    "x2_predict=model.predict(x2_shape)\n",
    "x2_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c664d3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = (train_generator.class_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4afcae6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'notperson': 0, 'person': 1}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4a542e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f734070",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6ddea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = tf.keras.models.load_model(argv[1])\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "# with tf.io.gfile.GFile('vww_96_float.tflite', 'wb') as float_file:\n",
    "#   float_file.write(tflite_model)\n",
    "\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "def representative_dataset_gen():\n",
    "  dataset_dir = os.path.join(BASE_DIR, \"person\")\n",
    "  for idx, image_file in enumerate(os.listdir(dataset_dir)):\n",
    "    # 10 representative images should be enough for calibration.\n",
    "    if idx > 10000:\n",
    "        return\n",
    "    full_path = os.path.join(dataset_dir, image_file)\n",
    "    if os.path.isfile(full_path):\n",
    "      img = tf.keras.preprocessing.image.load_img(\n",
    "          full_path, color_mode=\"grayscale\").resize((96, 96))\n",
    "      arr = tf.keras.preprocessing.image.img_to_array(img)\n",
    "      # Scale input to [0, 1.0] like in training.\n",
    "      yield [arr.reshape(1, 96, 96, 1)]\n",
    "      #yield [arr.reshape(1, 96, 96, 1) / 255.] \n",
    "\n",
    "# Convert model to full-int8 and save as quantized tflite flatbuffer.\n",
    "converter.representative_dataset = representative_dataset_gen\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.target_spec.supported_types = [tf.float16]\n",
    "quantized_tflite_model = converter.convert()\n",
    "with tf.io.gfile.GFile('vww_96_flaot.tflite', 'wb') as quantized_file:\n",
    "    quantized_file.write(quantized_tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22bc7eb1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
